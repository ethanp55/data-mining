{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris, load_wine\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore the warning messages from sklearn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets and random seeds we will use\n",
    "x_iris, y_iris = load_iris(return_X_y=True)\n",
    "x_wine, y_wine = load_wine(return_X_y=True)\n",
    "\n",
    "datasets = [('iris', x_iris, y_iris), ('wine', x_wine, y_wine)]\n",
    "random_seeds = [100, 500, 1000, 5000, 10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate COD\n",
    "def cod(h1, h2, true):\n",
    "    assert len(h1) == len(h2) == len(true)\n",
    "\n",
    "    n = len(true)\n",
    "    n10 = ((h1 == true) & (h1 != h2)).sum()\n",
    "    n01 = ((h2 == true) & (h2 != h1)).sum()\n",
    "    n00_d = ((h1 != true) & (h2 != true) & (h1 != h2)).sum()\n",
    "\n",
    "    return (n10 + n01 + n00_d) / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_results = []\n",
    "\n",
    "# Obtain results on each dataset\n",
    "for dataset_name, x, y in datasets:\n",
    "    trained_models = []\n",
    "\n",
    "    # Train versions of DT on different training sets\n",
    "    for random_seed in random_seeds:\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.75, random_state=random_seed)\n",
    "        dt = DecisionTreeClassifier()\n",
    "        dt.fit(x_train, y_train)\n",
    "        \n",
    "        trained_models.append((dt, x_test, y_test))\n",
    "\n",
    "    # Get COD scores for each pair of trained DT models\n",
    "    results = []\n",
    "\n",
    "    for i in range(len(trained_models) - 1):\n",
    "        model_i, test_set, labels = trained_models[i]\n",
    "\n",
    "        for j in range(i + 1, len(trained_models)):\n",
    "            model_j, _, _ = trained_models[j]\n",
    "            pred_i, pred_j = model_i.predict(test_set), model_j.predict(test_set)\n",
    "            result = cod(pred_i, pred_j, labels)\n",
    "\n",
    "            results.append((i, j, result))\n",
    "\n",
    "    dataset_results.append((dataset_name, results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on iris:\n",
      "COD on models 1 and 2: 0.05263157894736842\n",
      "COD on models 1 and 3: 0.02631578947368421\n",
      "COD on models 1 and 4: 0.02631578947368421\n",
      "COD on models 1 and 5: 0.02631578947368421\n",
      "COD on models 2 and 3: 0.0\n",
      "COD on models 2 and 4: 0.02631578947368421\n",
      "COD on models 2 and 5: 0.0\n",
      "COD on models 3 and 4: 0.07894736842105263\n",
      "COD on models 3 and 5: 0.05263157894736842\n",
      "COD on models 4 and 5: 0.07894736842105263\n",
      "\n",
      "Results on wine:\n",
      "COD on models 1 and 2: 0.15555555555555556\n",
      "COD on models 1 and 3: 0.13333333333333333\n",
      "COD on models 1 and 4: 0.17777777777777778\n",
      "COD on models 1 and 5: 0.2\n",
      "COD on models 2 and 3: 0.044444444444444446\n",
      "COD on models 2 and 4: 0.08888888888888889\n",
      "COD on models 2 and 5: 0.06666666666666667\n",
      "COD on models 3 and 4: 0.1111111111111111\n",
      "COD on models 3 and 5: 0.13333333333333333\n",
      "COD on models 4 and 5: 0.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the COD scores for each dataset and model pair\n",
    "for dataset_name, results in dataset_results:\n",
    "    print(F'Results on {dataset_name}:')\n",
    "\n",
    "    for i, j, result in results:\n",
    "        print(f'COD on models {i + 1} and {j + 1}: {result}')\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar approach as Part 1, but this time with 1 dataset and 2 different models\n",
    "\n",
    "# Models we will use and final results (initialized to empty)\n",
    "models, model_results = [('DT', DecisionTreeClassifier), ('MLP', MLPClassifier)], []\n",
    "\n",
    "# Obtain results for each model\n",
    "for model_name, model_type in models:\n",
    "    trained_models = []\n",
    "\n",
    "    # Train versions of the model on different training sets\n",
    "    for random_seed in random_seeds:\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_wine, y_wine, train_size=0.75, random_state=random_seed)\n",
    "        model = model_type()\n",
    "        model.fit(x_train, y_train)\n",
    "        \n",
    "        trained_models.append((model, x_test, y_test))\n",
    "\n",
    "    # Get COD scores for each pair of trained models\n",
    "    results = []\n",
    "\n",
    "    for i in range(len(trained_models) - 1):\n",
    "        model_i, test_set, labels = trained_models[i]\n",
    "\n",
    "        for j in range(i + 1, len(trained_models)):\n",
    "            model_j, _, _ = trained_models[j]\n",
    "            pred_i, pred_j = model_i.predict(test_set), model_j.predict(test_set)\n",
    "            result = cod(pred_i, pred_j, labels)\n",
    "\n",
    "            results.append((i, j, result))\n",
    "\n",
    "    model_results.append((model_name, results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for DT:\n",
      "COD on models 1 and 2: 0.2\n",
      "COD on models 1 and 3: 0.17777777777777778\n",
      "COD on models 1 and 4: 0.2222222222222222\n",
      "COD on models 1 and 5: 0.2\n",
      "COD on models 2 and 3: 0.06666666666666667\n",
      "COD on models 2 and 4: 0.08888888888888889\n",
      "COD on models 2 and 5: 0.08888888888888889\n",
      "COD on models 3 and 4: 0.13333333333333333\n",
      "COD on models 3 and 5: 0.06666666666666667\n",
      "COD on models 4 and 5: 0.24444444444444444\n",
      "\n",
      "Results for MLP:\n",
      "COD on models 1 and 2: 0.7111111111111111\n",
      "COD on models 1 and 3: 0.7111111111111111\n",
      "COD on models 1 and 4: 0.9111111111111111\n",
      "COD on models 1 and 5: 0.1111111111111111\n",
      "COD on models 2 and 3: 0.7333333333333333\n",
      "COD on models 2 and 4: 0.8444444444444444\n",
      "COD on models 2 and 5: 0.8\n",
      "COD on models 3 and 4: 0.8666666666666667\n",
      "COD on models 3 and 5: 0.6888888888888889\n",
      "COD on models 4 and 5: 0.8666666666666667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the COD scores for each model type and pair\n",
    "for model_name, results in model_results:\n",
    "    print(F'Results for {model_name}:')\n",
    "\n",
    "    for i, j, result in results:\n",
    "        print(f'COD on models {i + 1} and {j + 1}: {result}')\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For part 1 I used a Decision Tree (DT) for the classification algorithm.  I also used 5 different random seeds for splitting the datasets into train and tests sets, as well as the iris and wine datasets.  For each training set, I fit a model and then used COD to compare all of the pairs of models (as instructed).  From both the paper and intuition, COD reflects the probability that two models will make different predictions.  By looking at the results at the end of the code in part 1, one can see that nearly all of the COD scores/probabilities are low, which indicates that <b style=\"color:red;\">DT likely has a good amount of bias</b> (i.e. regardless of the training set, DT tends to make the same predictions and errors). <br>\n",
    "\n",
    "For part 2, I used the wine dataset, DT again for the non-stochastic algorithm, and MLP for the stochastic algorithm.  Again, I used 5 random seeds to obtain different training and test sets, fit different versions of these models on those training sets, and compared the models by calculating COD scores.  One can see from the output that the COD values for DT are fairly low, whereas the values for MLP are significantly higher.  This indicates that <b style=\"color:red;\">DT probably has stronger bias than MLP and MLP has more variance</b> (which seems to make sense due to the more stochastic nature of MLP, combined with the results we saw for DT in part 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 ('forex')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2bc6dffc417b633bbbc31cedd954f3e10eebcdab1341647d4de83cb692948a0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
