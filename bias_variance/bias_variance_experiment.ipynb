{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris, load_wine\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore the warning messages from sklearn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets and random seeds we will use\n",
    "x_iris, y_iris = load_iris(return_X_y=True)\n",
    "x_wine, y_wine = load_wine(return_X_y=True)\n",
    "\n",
    "datasets = [('iris', x_iris, y_iris), ('wine', x_wine, y_wine)]\n",
    "random_seeds = [100, 500, 1000, 5000, 10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate COD\n",
    "def cod(h1, h2, true):\n",
    "    assert len(h1) == len(h2) == len(true)\n",
    "\n",
    "    n = len(true)\n",
    "    n10 = ((h1 == true) & (h1 != h2)).sum()\n",
    "    n01 = ((h2 == true) & (h2 != h1)).sum()\n",
    "    n00_d = ((h1 != true) & (h2 != true) & (h1 != h2)).sum()\n",
    "\n",
    "    return (n10 + n01 + n00_d) / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_results = []\n",
    "\n",
    "# Obtain results on each dataset\n",
    "for dataset_name, x, y in datasets:\n",
    "    trained_models = []\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.75)\n",
    "\n",
    "    # Train versions of DT on different training sets\n",
    "    for random_seed in random_seeds:\n",
    "        x_train_new, _, y_train_new, _ = train_test_split(x_train, y_train, train_size=0.75, random_state=random_seed)\n",
    "        dt = DecisionTreeClassifier()\n",
    "        dt.fit(x_train_new, y_train_new)\n",
    "        \n",
    "        trained_models.append(dt)\n",
    "\n",
    "    # Get COD scores for each pair of trained DT models\n",
    "    results = []\n",
    "\n",
    "    for i in range(len(trained_models) - 1):\n",
    "        model_i = trained_models[i]\n",
    "\n",
    "        for j in range(i + 1, len(trained_models)):\n",
    "            model_j = trained_models[j]\n",
    "            pred_i, pred_j = model_i.predict(x_test), model_j.predict(x_test)\n",
    "            result = cod(pred_i, pred_j, y_test)\n",
    "\n",
    "            results.append((i, j, result))\n",
    "\n",
    "    dataset_results.append((dataset_name, results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on iris:\n",
      "COD on models 1 and 2: 0.05263157894736842\n",
      "COD on models 1 and 3: 0.05263157894736842\n",
      "COD on models 1 and 4: 0.05263157894736842\n",
      "COD on models 1 and 5: 0.05263157894736842\n",
      "COD on models 2 and 3: 0.0\n",
      "COD on models 2 and 4: 0.0\n",
      "COD on models 2 and 5: 0.0\n",
      "COD on models 3 and 4: 0.0\n",
      "COD on models 3 and 5: 0.0\n",
      "COD on models 4 and 5: 0.0\n",
      "\n",
      "Results on wine:\n",
      "COD on models 1 and 2: 0.17777777777777778\n",
      "COD on models 1 and 3: 0.1111111111111111\n",
      "COD on models 1 and 4: 0.1111111111111111\n",
      "COD on models 1 and 5: 0.08888888888888889\n",
      "COD on models 2 and 3: 0.1111111111111111\n",
      "COD on models 2 and 4: 0.1111111111111111\n",
      "COD on models 2 and 5: 0.08888888888888889\n",
      "COD on models 3 and 4: 0.0\n",
      "COD on models 3 and 5: 0.022222222222222223\n",
      "COD on models 4 and 5: 0.022222222222222223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the COD scores for each dataset and model pair\n",
    "for dataset_name, results in dataset_results:\n",
    "    print(F'Results on {dataset_name}:')\n",
    "\n",
    "    for i, j, result in results:\n",
    "        print(f'COD on models {i + 1} and {j + 1}: {result}')\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar approach as Part 1, but this time with 1 dataset and 2 different models\n",
    "\n",
    "# Models we will use and final results (initialized to empty)\n",
    "models, model_results = [('DT', DecisionTreeClassifier), ('MLP', MLPClassifier)], []\n",
    "\n",
    "# Split the dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_wine, y_wine, train_size=0.75)\n",
    "\n",
    "# Obtain results for each model\n",
    "for model_name, model_type in models:\n",
    "    trained_models = []\n",
    "\n",
    "    # Train versions of the model on the same training set\n",
    "    for _ in random_seeds:\n",
    "        model = model_type()\n",
    "        model.fit(x_train, y_train)\n",
    "        \n",
    "        trained_models.append(model)\n",
    "\n",
    "    # Get COD scores for each pair of trained models\n",
    "    results = []\n",
    "\n",
    "    for i in range(len(trained_models) - 1):\n",
    "        model_i = trained_models[i]\n",
    "\n",
    "        for j in range(i + 1, len(trained_models)):\n",
    "            model_j = trained_models[j]\n",
    "            pred_i, pred_j = model_i.predict(x_test), model_j.predict(x_test)\n",
    "            result = cod(pred_i, pred_j, y_test)\n",
    "\n",
    "            results.append((i, j, result))\n",
    "\n",
    "    model_results.append((model_name, results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for DT:\n",
      "COD on models 1 and 2: 0.1111111111111111\n",
      "COD on models 1 and 3: 0.0\n",
      "COD on models 1 and 4: 0.1111111111111111\n",
      "COD on models 1 and 5: 0.1111111111111111\n",
      "COD on models 2 and 3: 0.1111111111111111\n",
      "COD on models 2 and 4: 0.0\n",
      "COD on models 2 and 5: 0.0\n",
      "COD on models 3 and 4: 0.1111111111111111\n",
      "COD on models 3 and 5: 0.1111111111111111\n",
      "COD on models 4 and 5: 0.0\n",
      "\n",
      "Results for MLP:\n",
      "COD on models 1 and 2: 0.7555555555555555\n",
      "COD on models 1 and 3: 0.1111111111111111\n",
      "COD on models 1 and 4: 0.35555555555555557\n",
      "COD on models 1 and 5: 0.7555555555555555\n",
      "COD on models 2 and 3: 0.7777777777777778\n",
      "COD on models 2 and 4: 0.8444444444444444\n",
      "COD on models 2 and 5: 0.0\n",
      "COD on models 3 and 4: 0.3333333333333333\n",
      "COD on models 3 and 5: 0.7777777777777778\n",
      "COD on models 4 and 5: 0.8444444444444444\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the COD scores for each model type and pair\n",
    "for model_name, results in model_results:\n",
    "    print(F'Results for {model_name}:')\n",
    "\n",
    "    for i, j, result in results:\n",
    "        print(f'COD on models {i + 1} and {j + 1}: {result}')\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For part 1 I used a Decision Tree (DT) for the classification algorithm.  I also used 5 different random seeds for selecting from the training set, as well as the iris and wine datasets.  For each training set, I fit a model and then used COD to compare all of the pairs of models (as instructed).  From both the paper and intuition, COD reflects the probability that two models will make different predictions.  By looking at the results at the end of the code in part 1, one can see that nearly all of the COD scores/probabilities are low, which indicates that <b style=\"color:red;\">DT likely has a good amount of bias</b> (i.e. regardless of the training set, DT tends to make the same predictions and errors). <br>\n",
    "\n",
    "For part 2, I used the wine dataset, DT again for the non-stochastic algorithm, and MLP for the stochastic algorithm.  However, this time I fit multiple models using the same training set.  One can see from the output that the COD values for DT are essentially 0, whereas the values for MLP are significantly higher.  This indicates that <b style=\"color:red;\">DT probably has stronger bias than MLP and MLP has more variance</b> (which seems to make sense due to the more stochastic nature of MLP, combined with the results we saw for DT in part 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 ('forex')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2bc6dffc417b633bbbc31cedd954f3e10eebcdab1341647d4de83cb692948a0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
